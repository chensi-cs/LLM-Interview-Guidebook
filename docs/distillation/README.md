# ğŸ«— çŸ¥è¯†è’¸é¦

## ğŸ¯ æ¦‚è¿°
çŸ¥è¯†è’¸é¦é€šè¿‡æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶ï¼Œå°†å¤§æ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°å°æ¨¡å‹ï¼Œå®ç°æ€§èƒ½ä¸æ•ˆç‡çš„å¹³è¡¡ã€‚

## ğŸ—ï¸ è’¸é¦ç±»å‹

### 1ï¸âƒ£ å“åº”è’¸é¦ (Response Distillation)
- **åŸç†**ï¼šåŒ¹é…æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹çš„è¾“å‡ºlogits
- **æŸå¤±å‡½æ•°**ï¼š$\mathcal{L}_{KD} = \alpha T^2 \cdot \text{KL}(p_T^T || p_S^T) + (1-\alpha) \cdot \text{CE}(y, p_S)$
- **æ¸©åº¦T**ï¼šæ§åˆ¶softmaxå¹³æ»‘åº¦

### 2ï¸âƒ£ ç‰¹å¾è’¸é¦ (Feature Distillation)
- **åŸç†**ï¼šåŒ¹é…ä¸­é—´å±‚ç‰¹å¾è¡¨ç¤º
- **æ–¹æ³•**ï¼šHinton Lossã€FitNetsã€Attention Transfer
- **ä¼˜åŠ¿**ï¼šä¿ç•™æ›´å¤šè¯­ä¹‰ä¿¡æ¯

### 3ï¸âƒ£ å…³ç³»è’¸é¦ (Relational Distillation)
- **åŸç†**ï¼šåŒ¹é…æ ·æœ¬é—´å…³ç³»
- **æ–¹æ³•**ï¼šRKDã€CRDã€Relational Knowledge Distillation

## ğŸ“Š è’¸é¦ç­–ç•¥å¯¹æ¯”
| ç±»å‹ | ä¿¡æ¯æ¥æº | æ•ˆæœ | è®¡ç®—æˆæœ¬ | é€‚ç”¨åœºæ™¯ |
|---|---|---|---|---|
| **å“åº”è’¸é¦** | è¾“å‡ºå±‚ | ä¸­ | ä½ | å¿«é€Ÿéƒ¨ç½² |
| **ç‰¹å¾è’¸é¦** | ä¸­é—´å±‚ | å¥½ | ä¸­ | ç²¾åº¦ä¼˜å…ˆ |
| **å…³ç³»è’¸é¦** | æ ·æœ¬å…³ç³» | å¾ˆå¥½ | é«˜ | å¤æ‚ä»»åŠ¡ |

## ğŸ¯ å®æˆ˜æ¡ˆä¾‹
```python
class DistillationLoss(nn.Module):
    def __init__(self, temperature=4.0, alpha=0.7):
        super().__init__()
        self.T = temperature
        self.alpha = alpha
        
    def forward(self, student_logits, teacher_logits, labels):
        # è’¸é¦æŸå¤±
        soft_targets = F.softmax(teacher_logits / self.T, dim=-1)
        soft_pred = F.log_softmax(student_logits / self.T, dim=-1)
        distill_loss = F.kl_div(soft_pred, soft_targets, reduction='batchmean') * (self.T ** 2)
        
        # å­¦ç”ŸæŸå¤±
        student_loss = F.cross_entropy(student_logits, labels)
        
        return self.alpha * distill_loss + (1 - self.alpha) * student_loss
```

## ğŸ¯ é¢è¯•é‡ç‚¹
1. **çŸ¥è¯†è’¸é¦çš„ä¸‰ä¸ªå±‚æ¬¡ï¼Ÿ**
2. **æ¸©åº¦å‚æ•°çš„ä½œç”¨ï¼Ÿ**
3. **å¦‚ä½•é€‰æ‹©è’¸é¦ç›®æ ‡ï¼Ÿ**
4. **è’¸é¦ä¸å‰ªæã€é‡åŒ–çš„åŒºåˆ«ï¼Ÿ**