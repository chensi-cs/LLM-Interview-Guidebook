# ğŸ“Š TransformeråŸºç¡€ç»“æ„

## ğŸ¯ æ¦‚è¿°
Transformeræ˜¯ä¸€ç§åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç”±Vaswaniç­‰äººåœ¨2017å¹´æå‡ºï¼Œå½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸã€‚

## ğŸ—ï¸ æ ¸å¿ƒç»„ä»¶

### 1ï¸âƒ£ ç¼–ç å™¨-è§£ç å™¨æ¶æ„
- **ç¼–ç å™¨**ï¼šå°†è¾“å…¥åºåˆ—è½¬æ¢ä¸ºéšè—è¡¨ç¤º
- **è§£ç å™¨**ï¼šåŸºäºç¼–ç å™¨è¾“å‡ºç”Ÿæˆç›®æ ‡åºåˆ—

### 2ï¸âƒ£ å…³é”®åˆ›æ–°
- **è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼šå¹¶è¡Œå¤„ç†åºåˆ—ï¼Œæ•è·é•¿è·ç¦»ä¾èµ–
- **ä½ç½®ç¼–ç **ï¼šä¸ºæ¨¡å‹æä¾›åºåˆ—ä½ç½®ä¿¡æ¯
- **æ®‹å·®è¿æ¥**ï¼šç¼“è§£æ·±å±‚ç½‘ç»œè®­ç»ƒé—®é¢˜
- **å±‚å½’ä¸€åŒ–**ï¼šç¨³å®šè®­ç»ƒè¿‡ç¨‹

## ğŸ“‹ æ¶æ„è¯¦è§£

### ç¼–ç å™¨ç»“æ„
æ¯ä¸ªç¼–ç å™¨å±‚åŒ…å«ï¼š
1. **å¤šå¤´è‡ªæ³¨æ„åŠ›**ï¼šè®¡ç®—è¾“å…¥åºåˆ—å†…éƒ¨å…³ç³»
2. **å‰é¦ˆç¥ç»ç½‘ç»œ**ï¼šéçº¿æ€§å˜æ¢
3. **æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–**

### è§£ç å™¨ç»“æ„
æ¯ä¸ªè§£ç å™¨å±‚åŒ…å«ï¼š
1. **æ©ç å¤šå¤´è‡ªæ³¨æ„åŠ›**ï¼šé˜²æ­¢ä¿¡æ¯æ³„éœ²
2. **ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›**ï¼šå…³æ³¨è¾“å…¥åºåˆ—
3. **å‰é¦ˆç¥ç»ç½‘ç»œ**
4. **æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–**

## ğŸ” æ•°å­¦åŸç†

### ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
$$
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

### å¤šå¤´æ³¨æ„åŠ›
$$
\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1,...,\text{head}_h)W^O
$$
å…¶ä¸­ $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$

## ğŸš€ ä»£ç ç¤ºä¾‹

```python
import torch
import torch.nn as nn

class TransformerBlock(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attention = nn.MultiheadAttention(d_model, n_heads)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Linear(d_ff, d_model)
        )
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        # è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥
        attn_output, _ = self.attention(x, x, x, attn_mask=mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x
```

## ğŸ“š æ·±å…¥é˜…è¯»
- [åŸå§‹è®ºæ–‡ï¼šAttention Is All You Need](https://arxiv.org/abs/1706.03762)
- [åˆ†è¯å™¨è¯¦è§£](../tokenizer/README.md)
- [æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£](../attention/README.md)

## ğŸ¯ é¢è¯•é‡ç‚¹
1. **ä¸ºä»€ä¹ˆä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›ï¼Ÿ**
2. **ä½ç½®ç¼–ç çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ**
3. **æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–çš„ä½œç”¨ï¼Ÿ**
4. **Transformerç›¸æ¯”RNNçš„ä¼˜åŠ¿ï¼Ÿ**