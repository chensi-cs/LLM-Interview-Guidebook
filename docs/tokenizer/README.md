# ğŸ”¤ åˆ†è¯å™¨è¯¦è§£

## ğŸ¯ æ¦‚è¿°
åˆ†è¯å™¨(Tokenizers)æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ç†è§£çš„æ•°å­—åºåˆ—çš„å…³é”®ç»„ä»¶ï¼Œç›´æ¥å½±å“æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚

## ğŸ—ï¸ ä¸»æµåˆ†è¯ç®—æ³•

### 1ï¸âƒ£ BPE (Byte Pair Encoding)

**åŸç†**ï¼šé€šè¿‡åˆå¹¶é«˜é¢‘å­—ç¬¦å¯¹æ¥æ„å»ºè¯æ±‡è¡¨

**ä¼˜ç‚¹**ï¼š
- æœ‰æ•ˆå¤„ç†æœªç™»å½•è¯
- è¯æ±‡é‡å¯æ§
- å¤šè¯­è¨€æ”¯æŒå¥½

**ç¼ºç‚¹**ï¼š
- å¯èƒ½äº§ç”Ÿä¸å®Œæ•´çš„è¯
- å¯¹ä¸­æ–‡æ”¯æŒæœ‰é™

**å®ç°ç¤ºä¾‹**ï¼š
```python
from tokenizers import Tokenizer
from tokenizers.models import BPE

tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
```

### 2ï¸âƒ£ WordPiece

**åŸç†**ï¼šåŸºäºæœ€å¤§ä¼¼ç„¶ä¼°è®¡é€æ­¥åˆå¹¶è¯ç‰‡æ®µ

**ç‰¹ç‚¹**ï¼š
- Googleå¼€å‘ï¼Œç”¨äºBERT
- åœ¨è¯å‰æ·»åŠ `##`æ ‡è®°å­è¯
- æ›´é€‚åˆè‹±æ–‡

**ç¤ºä¾‹**ï¼š
```
"playing" -> ["play", "##ing"]
```

### 3ï¸âƒ£ SentencePiece

**åŸç†**ï¼šå°†æ–‡æœ¬è§†ä¸ºUnicodeåºåˆ—ï¼Œä¸ä¾èµ–ç©ºæ ¼åˆ†è¯

**ä¼˜åŠ¿**ï¼š
- è¯­è¨€æ— å…³æ€§
- æ”¯æŒä¸­æ–‡ã€æ—¥æ–‡ç­‰æ— ç©ºæ ¼è¯­è¨€
- å¯é€†è½¬æ¢

**é…ç½®ç¤ºä¾‹**ï¼š
```python
import sentencepiece as spm

spm.SentencePieceTrainer.train(
    input='input.txt',
    model_prefix='tokenizer',
    vocab_size=32000,
    model_type='bpe'
)
```

## ğŸ“Š ç®—æ³•å¯¹æ¯”

| ç‰¹æ€§ | BPE | WordPiece | SentencePiece |
|---|---|---|---|
| **åˆ†è¯ç²’åº¦** | å­è¯ | å­è¯ | å­è¯/å­—ç¬¦ |
| **è¯­è¨€æ”¯æŒ** | è‹±æ–‡ä¸ºä¸» | è‹±æ–‡ä¸ºä¸» | å¤šè¯­è¨€ |
| **ç©ºæ ¼å¤„ç†** | ä¾èµ–ç©ºæ ¼ | ä¾èµ–ç©ºæ ¼ | ä¸ä¾èµ–ç©ºæ ¼ |
| **è®­ç»ƒé€Ÿåº¦** | å¿« | ä¸­ç­‰ | æ…¢ |
| **æ¨¡å‹å¤§å°** | å° | ä¸­ç­‰ | å¤§ |

## ğŸ¯ å®æˆ˜åº”ç”¨

### ä¸­æ–‡åˆ†è¯æœ€ä½³å®è·µ
```python
# ä½¿ç”¨SentencePieceå¤„ç†ä¸­æ–‡
import sentencepiece as spm

# è®­ç»ƒä¸­æ–‡åˆ†è¯å™¨
spm.SentencePieceTrainer.train(
    input='chinese_corpus.txt',
    model_prefix='chinese_sp',
    vocab_size=32000,
    character_coverage=0.995,  # è¦†ç›–99.5%å­—ç¬¦
    model_type='bpe'
)

# ä½¿ç”¨åˆ†è¯å™¨
sp = spm.SentencePieceProcessor(model_file='chinese_sp.model')
tokens = sp.encode('å¤§æ¨¡å‹é¢è¯•å®å…¸', out_type=str)
print(tokens)  # ['å¤§', 'æ¨¡å‹', 'é¢è¯•', 'å®å…¸']
```

### è‹±æ–‡åˆ†è¯ç¤ºä¾‹
```python
# ä½¿ç”¨Hugging Face Tokenizers
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
tokens = tokenizer.tokenize("transformer architecture")
print(tokens)  # ['transform', '##er', 'arch', '##itecture']
```

## ğŸ” æŠ€æœ¯ç»†èŠ‚

### è¯æ±‡è¡¨æ„å»ºæµç¨‹
1. **é¢„å¤„ç†**ï¼šæ¸…æ´—æ–‡æœ¬ï¼Œæ ‡å‡†åŒ–
2. **è®­ç»ƒ**ï¼šåŸºäºè¯­æ–™åº“å­¦ä¹ åˆ†è¯è§„åˆ™
3. **éªŒè¯**ï¼šæ£€æŸ¥åˆ†è¯è´¨é‡
4. **ä¼˜åŒ–**ï¼šè°ƒæ•´è¶…å‚æ•°

### ç‰¹æ®Šæ ‡è®°å¤„ç†
- `[PAD]`ï¼šå¡«å……æ ‡è®°
- `[UNK]`ï¼šæœªçŸ¥è¯æ ‡è®°
- `[CLS]`ï¼šåˆ†ç±»æ ‡è®°
- `[SEP]`ï¼šåˆ†éš”æ ‡è®°
- `[MASK]`ï¼šæ©ç æ ‡è®°ï¼ˆç”¨äºMLMï¼‰

## ğŸ“š æ·±å…¥é˜…è¯»
- [æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£](../attention/README.md)
- [ä¸»æµå¤§æ¨¡å‹ç»“æ„](../models/README.md)

## ğŸ¯ é¢è¯•é‡ç‚¹
1. **BPEå’ŒWordPieceçš„åŒºåˆ«ï¼Ÿ**
2. **å¦‚ä½•å¤„ç†ä¸­æ–‡åˆ†è¯ï¼Ÿ**
3. **è¯æ±‡è¡¨å¤§å°å¦‚ä½•é€‰æ‹©ï¼Ÿ**
4. **OOV(æœªç™»å½•è¯)é—®é¢˜å¦‚ä½•è§£å†³ï¼Ÿ**