# ğŸ¯ æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£

## ğŸ¯ æ¦‚è¿°
æ³¨æ„åŠ›æœºåˆ¶æ˜¯Transformeræ¶æ„çš„æ ¸å¿ƒï¼Œå…è®¸æ¨¡å‹åœ¨å¤„ç†åºåˆ—æ—¶åŠ¨æ€åœ°å…³æ³¨é‡è¦ä¿¡æ¯ã€‚

## ğŸ—ï¸ æ³¨æ„åŠ›æœºåˆ¶ç±»å‹

### 1ï¸âƒ£ è‡ªæ³¨æ„åŠ›æœºåˆ¶ (Self-Attention, SA)

**åŸç†**ï¼šåºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ å…³æ³¨åºåˆ—ä¸­çš„å…¶ä»–æ‰€æœ‰å…ƒç´ 

**æ•°å­¦å…¬å¼**ï¼š
$$
\text{Self-Attention}(X) = \text{softmax}\left(\frac{XW_Q(XW_K)^T}{\sqrt{d_k}}\right)XW_V
$$

**ä»£ç ç¤ºä¾‹**ï¼š
```python
import torch
import torch.nn as nn

class SelfAttention(nn.Module):
    def __init__(self, d_model, d_k, d_v):
        super().__init__()
        self.w_q = nn.Linear(d_model, d_k)
        self.w_k = nn.Linear(d_model, d_k)
        self.w_v = nn.Linear(d_model, d_v)
        self.scale = torch.sqrt(torch.FloatTensor([d_k]))
    
    def forward(self, x, mask=None):
        Q = self.w_q(x)
        K = self.w_k(x)
        V = self.w_v(x)
        
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention = torch.softmax(scores, dim=-1)
        return torch.matmul(attention, V)
```

### 2ï¸âƒ£ äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ (Cross-Attention, CA)

**åŸç†**ï¼šä¸€ä¸ªåºåˆ—å…³æ³¨å¦ä¸€ä¸ªåºåˆ—çš„ä¿¡æ¯

**åº”ç”¨åœºæ™¯**ï¼š
- ç¼–ç å™¨-è§£ç å™¨æ¶æ„
- å¤šæ¨¡æ€èåˆ
- çŸ¥è¯†è’¸é¦

### 3ï¸âƒ£ å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ (Multi-Head Attention, MHA)

**åŸç†**ï¼šå¹¶è¡Œè¿è¡Œå¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œæ•è·ä¸åŒç±»å‹çš„å…³ç³»

**æ¶æ„**ï¼š
```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # çº¿æ€§å˜æ¢å¹¶åˆ†å¤´
        Q = self.w_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.w_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.w_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        
        # æ³¨æ„åŠ›è®¡ç®—
        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.FloatTensor([self.d_k]))
        
        if mask is not None:
            mask = mask.unsqueeze(1).unsqueeze(1)
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention = torch.softmax(scores, dim=-1)
        context = torch.matmul(attention, V)
        
        # åˆå¹¶å¤šå¤´
        context = context.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model
        )
        
        return self.w_o(context)
```

### 4ï¸âƒ£ åˆ†ç»„æ³¨æ„åŠ›æœºåˆ¶ (Grouped Query Attention, GQA)

**åŸç†**ï¼šå°†æŸ¥è¯¢å¤´åˆ†ç»„ï¼Œæ¯ç»„å…±äº«é”®å€¼å¤´ï¼Œå¹³è¡¡MHAå’ŒMQA

**ä¼˜åŠ¿**ï¼š
- å‡å°‘å†…å­˜å¸¦å®½éœ€æ±‚
- ä¿æŒæ¨¡å‹è´¨é‡
- æ¨ç†åŠ é€Ÿ

### 5ï¸âƒ£ å¤šæŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶ (Multi-Query Attention, MQA)

**åŸç†**ï¼šæ‰€æœ‰æŸ¥è¯¢å¤´å…±äº«ç›¸åŒçš„é”®å€¼å¤´

**ç‰¹ç‚¹**ï¼š
- æ˜¾è‘—å‡å°‘å†…å­˜å¸¦å®½
- æ¨ç†é€Ÿåº¦æå‡
- å¯èƒ½è½»å¾®å½±å“è´¨é‡

### 6ï¸âƒ£ å¤šå¤´æ½œåœ¨æ³¨æ„åŠ› (Multi-Head Latent Attention, MLA)

**åŸç†**ï¼šé€šè¿‡ä½ç§©æŠ•å½±å‡å°‘é”®å€¼ç¼“å­˜

**DeepSeekåˆ›æ–°**ï¼š
- ä½ç§©é”®å€¼è”åˆå‹ç¼©
- å‡å°‘æ¨ç†æ—¶KVç¼“å­˜
- ä¿æŒè¡¨è¾¾èƒ½åŠ›

## ğŸ“Š æ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”

| æœºåˆ¶ | å‚æ•°é‡ | å†…å­˜å ç”¨ | æ¨ç†é€Ÿåº¦ | è´¨é‡ |
|---|---|---|---|---|
| **MHA** | é«˜ | é«˜ | æ…¢ | é«˜ |
| **GQA** | ä¸­ | ä¸­ | ä¸­ | é«˜ |
| **MQA** | ä½ | ä½ | å¿« | ä¸­ |
| **MLA** | ä½ | æä½ | å¿« | é«˜ |

## ğŸ¯ é¢è¯•é‡ç‚¹

### é«˜é¢‘é—®é¢˜
1. **è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›çš„åŒºåˆ«ï¼Ÿ**
2. **ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´æ³¨æ„åŠ›ï¼Ÿ**
3. **GQAå’ŒMQAçš„æƒè¡¡ï¼Ÿ**
4. **å¦‚ä½•è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Ÿ**
5. **æ³¨æ„åŠ›æœºåˆ¶çš„æ—¶é—´å’Œç©ºé—´å¤æ‚åº¦ï¼Ÿ**

### å®æˆ˜åˆ†æ
```python
# è®¡ç®—æ³¨æ„åŠ›å¤æ‚åº¦
def attention_complexity(seq_len, d_model, n_heads):
    # è®¡ç®—æ³¨æ„åŠ›çŸ©é˜µ: O(nÂ²d)
    # å­˜å‚¨KVç¼“å­˜: O(nhd)
    time_complexity = seq_len * seq_len * d_model
    space_complexity = seq_len * n_heads * (d_model // n_heads)
    return time_complexity, space_complexity
```

## ğŸ“š æ·±å…¥é˜…è¯»
- [TransformeråŸºç¡€ç»“æ„](../transformer/README.md)
- [ä½ç½®ç¼–ç è¯¦è§£](../position/README.md)
- [ä¸»æµå¤§æ¨¡å‹ç»“æ„](../models/README.md)