# 🎮 强化学习

## 🎯 概述
强化学习(RL)在大模型中主要用于对齐训练，使模型行为符合人类价值观和偏好。

## 🏗️ 核心算法

### 1. 背景

#### 1）数学期望

**期望值**是随机变量所有可能取值的加权平均。它表示长期来看，某个随机变量的平均结果。强化学习中的价值函数 V(s)和Q(s,a) 本质上是期望值，因为它们考虑了所有可能的未来奖励。

#### 2）马尔可夫决策链

马尔可夫决策过程（Markov Decision Process，简称 MDP）是用于建模决策问题的数学框架，常用于决策学习。MDP是RL问题的数学形式化描述，MDP由以下几个组成部分构成：

1. **状态空间（State Space）**：所有可能的环境状态的集合。状态是对环境当前情况的描述。

2. **动作空间（Action Space）**：智能体在每个状态下可以选择的所有可能动作的集合。

3. **转移概率（Transition Probability）**：描述了在当前状态下采取某个动作后转移到下一个状态的概率。通常用 P(s′∣s,a)表示，其中 s 是当前状态，a 是采取的动作，s′ 是下一个状态。

4. **奖励函数（Reward Function）**：给定当前状态和采取的动作后，智能体从环境中获得的即时奖励。通常用 R(s,a) 或 R(s,a,s′) 表示。（R(s, a, s') 表示在状态 ( s ) 下采取动作 ( a ) 并转移到状态 ( s' ) 时所获得的即时奖励）。

5. **策略（Policy）**：智能体在每个状态下选择动作的策略，通常用 π(a∣s) 表示，其中 π 是策略，a 是动作，s 是状态。策略可以是确定性的（在状态 s 下总是选择相同的动作）或随机的（在状态 s 下选择动作的概率分布）。

6. **折扣因子（Discount Factor）**：表示未来奖励的当前价值，通常用 γ 表示。折扣因子取值范围在 0 到 1 之间，其中 0 表示只考虑即时奖励，而接近 1 表示重视未来奖励。(未来的奖励不如现在等值的奖励那么好（比如一年后给100块不如现在就给），所以$r_{t+1}$的权重应该小于$r_t$ )

7. **MDP 目标**
   在MDP中，智能体的目标是找到一个策略 π、，使得从初始状态开始，智能体在整个过程中获得的累积奖励（折扣累积奖励）最大化。这个累积奖励通常被称为**回报（Return）**，计算公式为：$$R=\sum_{t=0}^{\infty} \gamma^t r_t$$ 。其中 $r_t$ 是在时间步 t 获得的奖励，$γ $ 是折扣因子。

#### 3）arg max

在数学和计算机科学中，**arg max** 是一个常见的符号，表示在给定函数下找到使函数值最大化的输入值（或参数）。

<img src="images/image-20250304005903920.png" alt="image-20250304005903920" style="zoom:67%;" />

### 2. 概念

强化学习（Reinforcement Learning, RL）是一种机器学习方法，它通过智能体（Agent）与环境（Environment）交互来学习策略（Policy），使得在不同状态（State）下采取的动作（Action）能够最大化累积奖励（Reward）。强化学习的核心思想是试错学习（Trial-and-Error Learning）和延迟奖励（Delayed Reward）。

**强化学习有几个关键概念：**

1. **智能体（Agent）**：做出决策的实体，它与环境进行交互。
2. **环境（Environment）**：智能体所处的外部系统，智能体的行动会影响环境的状态，环境会根据智能体的行为给予反馈。
3. **状态（State）**：环境在某一时刻的情况或配置，智能体基于这些状态做出决策。
4. **动作（Action）**：智能体在特定状态下可以采取的行为。
5. **奖励（Reward）**：智能体采取某个动作后，从环境中获得的反馈，奖励可以是正面的（促进目标的实现）或负面的（阻碍目标的实现）。
6. **策略（Policy）**：智能体在给定状态下选择动作的规则或函数，策略可以是确定性的也可以是随机的。
7. **价值函数（Value Function）**：衡量状态或动作的长期回报，评估某个状态或状态-动作对的好坏，价值函数帮助智能体了解在某个状态下采取行动的长期收益。

<img src="images/v2-625220ec74ca385f2aa8beea10eb0fdd_1440w.webp" alt="img" style="zoom: 33%;" />

**强化学习的特点：**

1. **无监督学习**：不需要标注数据，智能体通过试错学习。
2. **延迟奖励**：智能体的动作可能在未来才获得奖励。
3. **动态环境**：环境可能随时间变化，智能体需要适应。

**强化学习的目标**是找到最优策略，使得智能体在长远来看能够获得最大的累计奖励。

### 3.价值函数

强化学习中的环境由一个**马尔可夫决策过程（MDP）** 建模。

<img src="images/image-20250304003146534.png" alt="image-20250304003146534" style="zoom:67%;" />

<img src="images/image-20250304003245471.png" alt="image-20250304003245471" style="zoom:67%;" />




**贝尔曼方程（Bellman Equation）** 是强化学习的核心概念，它描述了一个状态的价值如何递归地依赖于 **当前奖励** 和 **未来状态的价值**。（**价值函数的核心公式是贝尔曼方程，它通过递归的方式计算长期收益**。）它的核心思想是：**当前的价值 = 直接奖励 + 未来可能的价值**。贝尔曼方程将价值函数和价值-动作对函数 **递归地表示** 为：

<img src="images/image-20250304003812712.png" alt="image-20250304003812712" style="zoom:67%;" />

<img src="images/image-20250304003835099.png" alt="image-20250304003835099" style="zoom:67%;" />

通过贝尔曼最优方程，可以推导出最优值函数，并进一步得到最优策略：

<img src="images/image-20250304004549573.png" alt="image-20250304004549573" style="zoom:67%;" />

<img src="images/image-20250304004608118.png" alt="image-20250304004608118" style="zoom:67%;" />

### 4.策略函数

策略函数是智能体（Agent）在某个状态 s 下选择动作 a 的规则。策略函数决定了智能体的行为方式

<img src="images/image-20250304011744803.png" alt="image-20250304011744803" style="zoom:67%;" />

<img src="images/image-20250304011820308.png" alt="image-20250304011820308" style="zoom:67%;" />

<img src="images/image-20250304011854079.png" alt="image-20250304011854079" style="zoom:67%;" />



### 5. 常见算法

强化学习的算法可以大致分为三类：

#### **基于价值（Value-based）方法**

**通过优化价值函数（如状态价值函数 V (s) 或状态-动作价值函数 *Q*(*s*,a)）来间接推导出最优策略，智能体首先找到最优的价值函数，然后通过价值函数选择能够带来最大长期回报的动作，策略通常通过贪心算法得到。**

价值函数可以表示每个状态的长期收益，动作价值函数则表示在特定状态下执行某个动作能获得的长期收益。

**代表算法**：

##### 1）**价值迭代（Value Iteration）**

价值迭代（Value Iteration）是一种动态规划（Dynamic Programming, DP）方法，用于求解马尔可夫决策过程（MDP）的最优策略。该方法通过反复更新价值函数，直至收敛到最优值，并从中提取最优策略。

<img src="images/image-20250304010155632.png" alt="image-20250304010155632" style="zoom:67%;" />

**价值迭代的基本流程如下：**

输入状态集合 S、动作集合 A、转移概率 P(s′∣s,a)、奖励函数 R(s,a)、折扣因子 γ、终止条件（如收敛误差 θ）

<img src="images/image-20250304010408574.png" alt="image-20250304010408574" style="zoom:67%;" />

#####  2）**Q-learning**

Q-learning 是一种值迭代算法，用于在 **无模型（model-free）** 的环境中学习最优策略。

![image-20250310151344415](images/image-20250310151344415.png)



#### **基于策略（Policy-based）方法**

##### Policy Gradient

策略梯度方法是一类直接优化策略 $π_θ(a∣s)$的方法，其核心思想是通过调整策略参数$θ$来最大化期望累积奖励，策略本身是参数化的函数（如神经网络），通过**梯度上升法**不断调整策略参数，以最大化累积累积奖励的期望



在此之前，首先介绍**策略梯度定理**，策略梯度定理是基于策略方法的理论基础，它提供了一种直接优化策略参数 $θ$ 的方法，以最大化期望累积奖励，累积奖励通常用**目标函数**$ J(θ) $表示：

![image-20250309221843221](images/image-20250309221843221.png)

![image-20250309222132426](images/image-20250309222132426.png)

应该是这样，下面会修正解释：

![image-20250309224140141](images/image-20250309224140141.png)

为什么**目标函数**$ J(θ) $对策略参数的梯度可以这么表示：

![image-20250309222409448](images/image-20250309222409448.png)

![image-20250309223414461](images/image-20250309223414461.png)

这里解释一下对数导数技巧，为什么可以那么写？

![image-20250309223527536](images/image-20250309223527536.png)

![image-20250309223635284](images/image-20250309223635284.png)

继续介绍策略梯度定理：

![image-20250309223804470](images/image-20250309223804470.png)

结论：提供了一种直接优化策略的方法，避免了基于值函数的间接优化，**核心是若某动作带来高累积奖励（ $G( \tau )$大），则通过梯度上升增大选择该动作的概率（$ \pi_{\theta} (a_t | s_t)$）；反之则减小其概率。**

![image-20250309224347044](images/image-20250309224347044.png)

##### REINFORCE 算法
经典的策略梯度方法

##### 策略梯度方法的问题

- **策略梯度方法的参数优化**
梯度上升法：$ \theta_{t+1} = \theta_{t} + \alpha \nabla_{\theta} J(\theta)$

- **策略梯度方法的采样：**
  **策略梯度中的样本** 指的是智能体与环境交互过程中产生的轨迹数据，具体来说是 “状态 - 动作 - 奖励” 的序列片段或完整轨迹。
  **样本的核心作用：** 用于估计策略梯度。策略梯度公式依赖这些样本中的 “动作对数概率” 和 “累积奖励”，通过它们计算梯度以更新策略。
  **采样** 是指智能体按照当前策略与环境实时交互，生成样本的过程。具体来说，是在每个状态下，根据策略输出的动作概率分布选择动作，并记录交互结果的过程。


- **传统策略的问题：**
  - **更新不稳定：** 每次参数更新前，都用当前最新的策略与环境交互采样新轨迹，例如：
第 1 轮：用初始策略 $π_{θ0}$ 采样轨迹，更新得到 $θ1$；
第 2 轮：用新策略  $π_{θ1}$ 重新采样轨迹，更新得到  $θ2$；
以此类推，每次采样的策略都是 “刚更新过的最新策略”。**但策略更新后，采样行为可能发生剧烈变化，导致样本分布不稳定。**
  -  **更新无约束：**策略更新幅度无约束，新旧策略的分布差异可能过大，导致基于旧策略采样的数据无法有效指导新策略学习
  - **样本方差大：** 策略梯度优化一个核心的假设是可以通过采用的方法来估计策略的梯度。但是当问题的规模变得非常大：比如每次轨迹都非常长，又或者策略模型非常大，为了预估准确的梯度，我们就不得不采样多次，否则就会面临方差很高的问题。

- 改进方法：
  - **减小方差：只关注未来，考虑当前动作之后的奖励**
    传统的策略梯度算法，无论当前在哪一步，总是会把整个轨迹中所有的 reward 都算进去。实际上，在评估当前走法时，你只需要关注从这一步开始直到局末的“后续表现”，即 “rewards to go” 。
    ![alt text](images/image.png)
  - **减小方差：参考线（Baseline）**
    为了进一步减少评估中的波动，我们可以为每一步的“后续奖励”减去一个基准值，在数学上表示为b，即参考线，他不一定是一个常数，更多时候是另外一个状态 $s_{t}$  的函数。这个参考显得实际意义是，在当前的状态下回报的期望值。那么所谓超出期望的部分就是优势（Advantage）。在实际训练中，我们会用优势代替原来的奖励进行梯度估计，以减小方差。
    ![alt text](images/image2.png)
  -  **减小方差：优势函数**
    “rewards to go” 即当前动作之后的奖励在强化学习中被称为 Q 函数（动作价值函数）$Q^\pi(s,a)$ ，即在状态 s 采取动作 a 后，未来能获得的累积奖励期望。$V^\pi(s)$是状态价值函数，表示 “在状态 s 下的累积奖励期望”（基准值）。然后，通过减去状态价值 $V^\pi(s)$ ,我们得到**优势函数：**
    $A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$,具体的含义是在某个状态 s 下，选择某个动作 a 相比于平均走法能提升多少胜率。如果这个动作带来的预期回报远高于当前的基准水平，那么这个动作的优势就是正的，说明它非常值得采用；反之，则说明不如平均水平。
    ![alt text](images/image3.png)
    优势函数在意的不是绝对的好，而是相对的好，即相对优势，$A^\pi(s,a)$通常可以由一个网络估计出来，成为“评论员Critic“

#### **基于 Actor-Critic（AC）方法**
Actor-Critic 方法结合了基于策略和基于价值的优点。在这种方法中：
  - Actor：代表策略部分，即想要训练的目标语言模型，负责生成动作。Actor 直接建模策略，并根据 Critic 的反馈调整策略参数，以优化长期奖励。
  - Critic 负责评估动作的价值，Critic 学习价值函数$V^\pi(s)$或动作价值函数$Q^\pi(s,a)$，并计算如 TD（时序差分）误差，这是当前估计的价值和实际获得的奖励之间的差异。
  - 两者协同优化，Actor-Critic 方法可以利用 Critic 的稳定性和快速收敛的特点，同时通过 Actor 学习更复杂的策略，包括在连续动作空间中的应用。此外，Critic 的反馈可以帮助减少 Actor 更新时的方差，从而提高学习的稳定性。


## 大模型后训练中的强化学习

### RLHF

RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）是一种结合强化学习和人类反馈的技术，核心是通过人类偏好指导模型优化，使输出更符合人类价值观、安全性和实用性

####  RLHF流程

RLHF 主要包括3个流程

1. **第一步：Supervised Fine-Tuning（SFT）监督微调**
用高质量的 “人类指令 - 响应” 数据微调预训练模型，得到初始模型 $model_{sft}$

1. **第二步：Reward Model（RM）奖励模型训练**
   - 让模型对同一指令生成多个候选答案，然后人工对后选答案进行质量排序或评分，构建偏好数据集
   - 用偏好数据集训练一个奖励模型来预测人类偏好得分
   - 
2. **第三步：Reinforcement Learning（PPO 等）优化策略模型**
   - 以初始模型 $model_{sft} $ 为初始策略，模型的输出由奖励模型打分，作为 RL 的 reward，用强化学习算法（PPO等）优化模型参数，目标是最大化奖励模型的打分（reward）
   - 约束：加入 “KL 散度惩罚”（KL Penalty），限制优化后的模型与 SFT 模型的偏离（避免模型忘记预训练知识或输出离谱内容）
   - 最终得到对齐人类偏好的模型 $model_{rlhf}$
  
### PPO（Proximal Policy Optimization，近端策略优化）
PPO 是 OpenAI 提出的强化学习算法，用于在 RLHF 中更新策略模型。
「近端」一词来源于优化问题中的近端优化（Proximal Optimization）概念。它指的是在优化过程中，通过引入某种约束或惩罚机制，限制每次更新的幅度，使得新策略不会偏离当前策略太远，从而保证优化的稳定性和安全性。

#### 动机
- 传统 RL 中的策略梯度方法容易出现 **更新太大导致性能崩坏** 的问题
- PPO 限制策略更新幅度，确保每次更新不会离旧策略太远

#### 核心思想
用 “新旧策略的概率比” 衡量策略变化，在限制策略更新幅度的同时进行优化，以达到稳定、高效的训练结果。


#### PPO类别
##### PPO-Clip（最常见的版本）
1. 纯策略梯度版本
  只有策略网络（近端比率裁剪损失），优势函数使用蒙特卡洛回报或TD估计，不学习价值函数

2. Actor-Critic版本（最常用）
  同时包含策略网络（Actor）和价值网络（Critic），使用价值网络估计状态价值函数，基于价值函数估计计算广义优势估计(GAE)，总损失函数通常包括策略损失、价值函数损失和熵正则化项

##### PPO-Penalty
这是PPO的另一个版本，使用KL散度惩罚而非裁剪：
$$ Loss_{penalty} = \mathbb{E}[ r_t(\theta)A_t - \beta KL(\pi_{\theta_{old}},\pi_{\theta})]$$
（具体公式先看下面的PPO-Clip-Actor-Critic就看得懂了）


#### PPO-Clip-Actor-Critic
PPO 的有两个关键组件：

- **裁剪机制（Clipping）：** 限制每次更新的幅度
- **价值网络（Value Network）：** 相当于评论员Critic，预测"从当前位置开始，预期能得多少分"，像一个经验丰富的老师，能预判学生的潜力

对应的，PPO算法使用了两个损失函数：
- 第一个损失函数是近端比率裁剪损失，用于限制策略更新幅度（最核心）；
- 第二个损失函数是价值函数损失，用于优化策略。


##### 优势函数的计算
$A_t = Q^\pi(s_t,a_t) - V^\pi(s_t)$ ，由于计算$  Q^\pi(s_t,a_t) $的难度较大，通常通过状态价值函数 $ V^\pi(s_t)$和即时奖励间接计算优势函数，主要有以下方式：

**1. 基础方法：时序差分法（TD,Temporal Difference）估计**
  为什么叫时序差分？因为它是相邻时间步的价值估计之间的差异，两个预测之间的差值，就是时间上的差分
  利用采样轨迹中的即时奖励 $r_t$,下一状态价值$ V^\pi(s_{t+1})$ 和折扣因子$\gamma $ 近似计算优势函数：

   - 通过 Critic 网络（价值网络）对当前状态 $s_t$ 的价值进行估计，得到当前状态的价值估计 $ V^\pi(s_t)$. 
   - 通过 Critic 网络（价值网络）对下一个状态 $s_{t+1}$ 的价值进行估计，得到下一个状态的价值估计 $ V^\pi(s_{t+1})$. 
   - 用即时奖励 $r_t$ 和下一状态价值估计 $ V^\pi(s_{t+1})$ 近似当前的动作价值$  Q^\pi(s_t,a_t) = r_t + \gamma V^\pi(s_{t+1})$,$ \gamma $是折扣因子
   - **优势函数**即为当前动作价值目标与当前状态价值的差值：$A_t =  r_t + \gamma V^\pi(s_{t+1}) - V^\pi(s_t)$ 
    这种方法也称为单步 TD 残差，计算简单但方差较大（仅依赖于单步回报来估计优势函数。由于即时奖励通常是随机的，这导致单步回报的方差较大，每次更新的估计值可能不够稳定，导致方差较大）
  
**2. 蒙特卡洛（MC）方法**
   ​蒙特卡洛方法通过从环境中采样完整的轨迹来估计累积奖励。即直接计算从当前状态到结束状态的累积奖励，再减去当前状态的价值函数估计。公式为：$A_t =  G_t - V^\pi(s_t)$ 
     - $ V^\pi(s_t)$是对当前状态 $s_t$ 的价值估计
     - $ G_t $ 是当前状态到结束状态的累积奖励，即$ G_t = r_t +  \gamma r_{t+1} +  \gamma r_{t+2} + ···$ ，$ \gamma $是折扣因子
   - 蒙特卡洛估计直接使用从环境中采样的完整轨迹来计算累积奖励，因此它是无偏差的。
   - 但由于每次估计都依赖于完整的轨迹，蒙特卡洛估计的方差较大，可能导致训练不稳定。


**3.  广义优势估计（GAE）​​**
  为了平衡偏差和方差，PPO 通常采用GAE（Generalized Advantage Estimation），通过引入参数 λ融合多步 TD 残差，公式定义如下：
  $$ A_t = \sum_{k=0}^{T}  ( {\gamma \lambda} )^ k  \delta _{t+k}$$
  - 其中$\delta _{t}= r_t + \gamma V^{\pi}(s_{t+1}) -  V^{\pi}(s_t) $是单步 TD 残差，$ \gamma $是折扣因子，$ \lambda $是GAE参数，用于控制多步残差的权重




##### 策略网络：近端比率裁剪损失
PPO算法中，策略网络用于学习和更新策略，这里使用了近端比率裁剪损失，用于限制策略更新幅度。近端比率裁剪损失定义如下：
$$ Loss_{clip} = \mathbb{E}[ min(r_t(\theta)A_t ,clip(r_t(\theta),1-\epsilon,1+\epsilon)A_t)]$$
其中：
- $r_t(\theta) = \frac{p_{\theta}(a_t | s_t)}{p_{old}(a_t | s_t)}$是策略的更新幅度,表示当前策略$\theta$在状态$s_t$下采取动作$a_t$的概率与旧策略$old$在状态$s_t$下采取动作$a_t$的概率之比,$r_t(\theta)$越大，表示在状态$s_t$下采取动作$a_t$的概率越大，即策略更新幅度越大
- $\epsilon$ 是超参数，用于控制裁剪幅度，通常取0.1~0.2
- $ clip(r_t(\theta),1-\epsilon,1+\epsilon)$ 表示将更新幅度$r_t(\theta)$限制在$ (1-\epsilon,1+\epsilon)$范围里
- $A_t = Q^\pi(s,a) - V^\pi(s)$ ：优势函数，表示在某个状态 s 下，选择某个动作 a 相比于平均水平能提升多少胜率（动作的相对收益，正为优，负为劣）
- 取最小值的原因也是限制策略更新幅度，可分为优势函数为正和负分别讨论



##### 价值网络：价值函数损失
价值函数（由 Critic 网络实现）的核心作用是：

- 估计状态 $s_t$ 的价值$ V^\pi(s_t)$ ，即从状态  $s_t$ 开始，遵循当前策略能获得的累积奖励期望”。

- 为优势函数 $A_t = Q^\pi(s_t,a_t) - V^\pi(s_t)$ 提供基准。（$  Q^\pi(s_t,a_t) = r_t + \gamma V^\pi(s_{t+1})$是动作价值，用即时奖励 $r_t$ 和下一状态价值估计 $ V^\pi(s_{t+1})$ 近似得到），而优势函数是策略优化的核心依据。

PPO 中价值函数损失通常采用均方误差（MSE）损失，目标是让 Critic 网络的预测值 $ V^\pi(s_t)$ 尽可能接近 “真实的累积奖励目标”，价值函数损失定义如下：

$$ Loss_{value} = \frac{1}{2} \mathbb{E}[(V_{\theta}(s_t) - V_{target}(s_t))^2]$$

其中，$V_{\theta}(s_t)$是Critic 网络（参数为 $\theta$）对状态 $s_t$  的价值预测（预计从状态  $s_t$ 开始，遵循当前策略能获得的累积奖励期望”）。
$ V_{target}(s_t)$是状态 $s_t$  的价值目标（真实累积奖励的估计）。

所以目标值$ V_{target}(s_t)$是从当前时刻 t 开始的所有未来奖励的折扣总和：$ V_{target}(s_t) =\sum_{k=t}^{T} \gamma^{t-k}r_k $,T是轨迹终止时间步。但**​​实际回报​​的计算存在一个核心矛盾**：当前时刻无法预知未来所有奖励，所以需要通过不同的方法​​合理估计​​实际回报。

​​(1) 蒙特卡洛（Monte Carlo, MC）方法​​
  - ​​适用场景​​：回合制任务（如游戏一局结束、机器人完成一次任务）。
  - 原理​​：等待轨迹完全结束后，从后向前计算实际回报。
  - 特点​​：
​     - ​无偏​​：使用真实未来奖励。
​     - ​高方差​​：依赖单条轨迹的随机性。

(2) 时序差分（Temporal Difference, TD）方法​​
  - ​适用场景​​：连续任务（如机器人持续控制、在线服务）。
  - 原理​​：用当前价值函数估计未来回报，无需等待轨迹结束。
  - 特点​​：
​     - ​有偏​​：依赖价值函数的准确性。
​     - ​低方差​​：减少了未来奖励的随机性影响。



(3) 广义优势估计（GAE）​​
  - ​原理​​：结合多步TD目标，平衡偏差与方差。


##### 损失函数
实际实现中，PPO的损失函数通常包含三部分：
$$ L_{total} = L_{策略函数损失} - c_1 L_{价值函数损失} + c_2S_{\theta} $$

其中，$ c_1 ,c_2$是超参数，用于平衡不同损失函数的权重，$S_{\theta}$是策略函数的熵，用于鼓励策略探索，防止策略过早收敛到单一动作


策略函数的熵$S_{\theta}$可以用来衡量策略的不确定性，即策略对于每个状态下的动作的概率分布的随机性。策略的熵越大，策略在每个状态下采取的动作的概率分布就越均匀，策略的探索性就越强。

#### PPO 在 RLHF 中的流程
在 RLHF 流程中，PPO 的工作流程如下：
1. 初始化：用 SFT 模型的权重初始化策略模型（Policy Model），并通常也用它来初始化价值模型（Value Model）。
2. 采样：从一个指令数据集中随机抽取一个指令（Prompt）。
3. 生成：策略模型根据指令生成一个回答。
4. 评估：奖励模型（RM）对“指令-回答”对打分，得到奖励（Reward）。
5. 价值模型估计状态 $s_t$ 的价值（Value）。
6. 计算优势：根据奖励和价值计算优势函数。
7. 更新：使用 PPO 的 Clipped Surrogate Objective 计算损失，并更新策略模型和价值模型的参数。循环：重复步骤 2-6，直到模型收敛。

### DPO（Direct Preference Optimization，直接偏好优化）

DPO 是 2023 年提出的简化对齐方法，直接用人类偏好数据优化模型，跳过了 RLHF 中的 “奖励模型训练” 和 “PPO 强化学习” 步骤

#### 背景
尽管 PPO 非常成功，但 RLHF 中的 PPO 流程相当复杂。它需要同时维护和训练多个模型（策略模型、价值模型、奖励模型、SFT 参考模型），这使得训练过程非常消耗计算资源和内存，且超参数调整也颇具挑战。正是这些挑战，催生了更简洁的替代方案——DPO。

#### 动机
传统 RLHF 是一个“两步走”的过程：先用偏好数据（A 比 B 好）训练一个能给绝对分数（A 得 90 分，B 得 60 分）的奖励模型，然后再用这个分数去指导强化学习。这个中间的奖励建模步骤不仅复杂，还可能引入误差。

DPO 直接跳过 显式奖励模型 和 RL 采样优化，用一个可解析的似然比目标从偏好数据直接优化策略模型，本质上是将偏好数据转化为一种直接的监督信号

#### 方法

1. 数据格式：DPO 使用 “成对比较数据”，即对于同一个输入（prompt），人类标注者会给出两个回答（优质回答， 地质回答），记为 $(x,y_{chosen},y_{reject} )$。
2. 目标函数：DPO 的目标是在输入相等的情况下，最大化 “偏好回答” 的概率，同时最小化 “非偏好回答” 的概率，通过以下损失函数实现：
3. 布拉德利-特里（Bradley-Terry） 偏好模型​，常用于预测成对比较的结果，对于任意两个对象 $i$ 和 $j$ ，若为每个对象分配正实数得分 $p_i$ 和  $p_j$ ,则对象被认为比对象强的概率为：$pr(i> j )= \frac{p_i}{p_i+p_j}$


4. DPO 基于布拉德利-特里（Bradley-Terry） 偏好模型​​， 假设人类的偏好概率 $p^*$ 可以用一个潜在的奖励模型 $r^*(y,x)$ 来建模：
    $$ p^*(y_w \succ y_l)=  \frac{r^*(y_w,x) }{r^*(y_w,x) + r^*(y_l,x)}  $$
其中，$y_w$ 指的是$y_{winner}$，即优质回答，$y_l$指的是$y_{loser}$，即低质回答。 $y_w \succ y_l$ 表示元素 $y_w$ 在某个偏序关系中严格大于（或优于）元素 $y_l$,  $ p^*(y_w \succ y_l) $表示策略偏向优质回答的概率，$y_w$  的奖励比 $y_l$高得越多，人类偏好$y_w$的概率就越大。
  
    为避免显式训练一个单独的奖励模型,将潜在的奖励模型 $r^*(y,x)$ 定义如下,其中 $\pi ^*$ 表示最佳策略（训练时用当前待更新模型 $\pi ^{\theta}$表示），$\pi ^{ref}$ 表示参考策略（当前策略和参考策略都来源于同一个初始模型，但是参考策略参数冻结）， $\beta$是超参数，用于缩放奖励。$\pi ^*(y,x)$ 表示给定x，策略输出y的概率
  $$ r^*(y,x) = \beta  log \frac{\pi ^*(y,x) }{\pi ^{ref}(y,x) } + const$$

  把假设的奖励表达式带入$ p^*(y_w \succ y_l) $ ，由下面的恒等式：
    $$log \frac{e^a}{e^a+e^b} = log \frac{1}{1+e^{b-a}} = log \alpha(a-b) $$
  其中$\alpha $表示sigmoid函数，就能得到一个直接可优化的对数似然目标：
  $$ Loss_{DPO}(\pi ^ {\theta} ;\pi ^ {ref}) = - log( \alpha (\beta (log \frac{\pi ^ {\theta} (y_w,x)}{\pi ^ {ref} (y_w,x) } - log \frac{\pi ^ {\theta} (y_l,x)}{\pi ^ {ref} (y_l,x) })))$$
  DPO希望最小化这个loss，实际上就是最大化$ p^*(y_w \succ y_l) $，即最大化 $y_w$ 被偏好的概率，理想情况，loss的计算中左半部分变大，右半部分变小，即chosen response概率提升，rejected response概率下降，模型训练结束后更倾向选择chosen的答案。
  左边的式子- 右边式子的值，即选择好回答和坏回答的差异：
  - 左边变大，右边变小，理想情况，good response概率提升，bad response概率下降
  - 左边变小，右边更小，good response概率下降，但是bad response概率下降的更多，生成的时候还是倾向于good response
  - 左边变的更大，右边只大了一点点，和2同理

#### DPO算法步骤

1. DPO算法仅包括两个模型：actor model 和 reference model
2. 微调过程开始时，首先对正在训练的model复制一份，并冻结其可训练参数，成为reference model
3. 对于每个样本，选择和拒绝的答案由训练和冻结的语言模型评分，这个评分是与每一步所需答案的所有 token 概率的乘积（或和）
4. 在对选择和拒绝的答案评分后，我们可以计算policy模型给出的评分（R_policy）与冻结reference模型给出的评分（R_reference）之间的比率。
5. 这些比率然后用于计算最终损失，该损失用于在梯度下降更新中修改模型权重。

#### GRPO（Group Relative Policy Optimization，群组相对策略优化）
GRPO（Group Relative Policy Optimization，群组相对策略优化）是一个基于 PPO 的变体，由 DeepSeek 团队在其论文《DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models》中首次提出

它的核心创新在于取消了 价值函数（Critic / Value Network），取而代之的是通过组内样本的相对奖励比较来估计优势（advantage），从而减少模型结构复杂度和算力开销



##### 背景

PPO 的核心是优势函数$A^\pi(s,a)$ ，它需要一个奖励模型来提供奖励$Q^\pi(s,a)$ ，还需要一个价值模型$V^\pi(s)$来提供基线，即在状态 $s_t$ 下的平均期望回报。训练和维护这个价值模型是 PPO 流程中主要的复杂性和成本来源之一。

##### 方法

GRPO 的核心是对 PPO 中优势函数的计算方式进行了修改。其步骤如下：
1. **组采样 (Group Sampling)**：对于一个给定的指令 q，使用当前的策略模型  $\pi ^{\theta}$ 生成一个包含多个回答的组  $o_1，o_2，···，o_n$ 。
2. **组评估 (Group Evaluation)**：使用一个奖励函数（可以是一个训练好的奖励模型，也可以是某种可计算的启发式规则，例如代码的执行结果、数学题的答案是否正确等）为组内的每一个回答  $o_i$ 打分，得到奖励 $r_i$。
3. 组内优势计算 (Group-Relative Advantage Estimation)：计算组内所有回答的平均奖励  $\bar{r}$ 和标准差 $\sigma_{r}$ 。对于组内的每一个回答 $o_i$ ，其优势被定义为其归一化后的奖励：$A_i= \frac{r_i- \bar{r}}{\sigma_r}$, 这种方法被称为组内奖励归一化（Group-wise Reward Normalization）。它直接用组内的统计量（均值和标准差）来替代了 PPO 中需要专门训练的价值模型所扮演的角色。
4. 策略更新：一旦计算出了每个样本的优势 $A_i$ ，接下来的步骤就和 PPO 非常相似了。GRPO 同样使用 Clipped Surrogate Objective 来更新策略模型,对组内每个输出分别计算策略比$r_t(\theta)$, （当前策略与旧策略的比值），然后使用 clip 优化目标：
   $$ Loss_{grpo-clip} = \mathbb{E}[ min(r_t(\theta)A_t ,clip(r_t(\theta),1-\epsilon,1+\epsilon)A_t)]$$

##### GRPO 的优势与特点
1. 高效性: GRPO 最显著的优势是无需价值模型。价值模型通常和策略模型一样大，去掉它可以节省近一半的训练内存和计算量，这对于训练超大规模模型来说意义重大。
2. 灵活性：GRPO 对奖励函数的定义非常灵活。它不一定需要一个端到端训练的神经网络奖励模型。在某些任务中（如代码生成、数学推理），我们可以设计出可验证的奖励函数（Verifiable Reward Functions）。例如，如果生成的代码能成功运行并通过所有单元测试，就给予高奖励；如果数学题的最终答案正确，也给予高奖励。这种方式使得奖励信号更客观、更廉价。
3. 稳定性：通过组内归一化，GRPO 使得优势函数的尺度保持在一个稳定的范围内，这有助于稳定训练过程，减少了对超参数的敏感性。

### 参考

[强化学习入门：基本思想和经典算法 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/466455380)

[从Policy Gradient到PPO的来龙去脉](https://www.cnblogs.com/yutian-blogs/p/17985176)


[强化学习7-PPO(Agent-only) 逐行代码讲解](https://zhuanlan.zhihu.com/p/624797778)


[近端策略优化 (PPO) 算法深度解析](https://mp.weixin.qq.com/s/gPEtblnP6Q7hGSa48HPHlg)

[DPO详解](https://mp.weixin.qq.com/s/HraJuI31IenMerzGKdml4w)

[强化学习入门，小学生都可以读懂的DPO，PPO](https://mp.weixin.qq.com/s/HBQNSMIhocWgrJ4R7W8qzQ)

[从 PPO、DPO 到 GRPO：万字长文详解大模型训练中的三大关键算法](https://mp.weixin.qq.com/s/OMpD6ITqNi4jX95nSRC2Ig)


## 📊 技术对比
| 方法 | 稳定性 | 样本效率 | 计算成本 | 应用 |
|---|---|---|---|---|
| **PPO** | 高 | 中 | 高 | ChatGPT |
| **DPO** | 中 | 高 | 低 | 简化RLHF |
| **RRHF** | 高 | 高 | 中 | 排名优化 |

## 🎯 面试重点
1. **RLHF的三个阶段是什么？**
2. **PPO如何防止策略崩溃？**
3. **奖励模型的训练目标？**
4. **RLHF与SFT的区别？**