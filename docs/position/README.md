# 📍 位置编码详解

## 🎯 概述
位置编码为Transformer提供序列位置信息，因为注意力机制本身不包含位置概念。

## 🏗️ 位置编码类型

### 1️⃣ 绝对位置编码

#### 可学习位置编码
- **原理**：将位置作为可训练参数
- **优点**：简单直接，可适应任务
- **缺点**：固定长度，泛化性差

#### Sinusoidal位置编码
- **原理**：使用正弦和余弦函数
- **公式**：
  $$
  PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
  $$
  $$
  PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
  $$

### 2️⃣ 相对位置编码

#### RoPE (旋转位置编码)
- **原理**：通过旋转矩阵编码相对位置
- **优点**：支持任意长度，相对位置感知
- **应用**：LLaMA、ChatGLM等

#### ALiBi (Attention with Linear Biases)
- **原理**：在注意力分数中添加线性偏置
- **优点**：外推能力强，计算高效
- **应用**：BLOOM、MPT等

## 📊 编码方法对比

| 方法 | 类型 | 外推能力 | 计算效率 | 应用模型 |
|---|---|---|---|---|
| **可学习** | 绝对 | 差 | 高 | 早期Transformer |
| **Sinusoidal** | 绝对 | 中 | 高 | 原始Transformer |
| **RoPE** | 相对 | 好 | 中 | LLaMA、Qwen |
| **ALiBi** | 相对 | 极好 | 高 | BLOOM、MPT |

## 🎯 面试重点
1. **为什么需要位置编码？**
2. **RoPE相比绝对位置编码的优势？**
3. **如何处理超出训练长度的序列？**