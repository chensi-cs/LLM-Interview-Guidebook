# 🔧 微调技术

## 🎯 微调概述
微调是将预训练模型适配到特定任务的关键技术，包括指令微调、对齐微调和高效参数微调。

## 🏗️ 微调类型

### 1️⃣ 指令微调 (SFT)
- **原理**：在指令-响应对上训练
- **数据格式**：{"instruction": "...", "output": "..."}
- **效果**：提升指令遵循能力

### 2️⃣ 对齐微调 (RLHF)
- **流程**：
  1. 人类偏好数据收集
  2. 奖励模型训练
  3. PPO强化学习优化
- **目标**：使模型行为符合人类价值观

### 3️⃣ 高效参数微调

#### LoRA (Low-Rank Adaptation)
- **原理**：低秩矩阵分解
- **公式**：$h = W_0x + \Delta Wx = W_0x + BAx$
- **优势**：参数量减少1000倍

#### Prefix Tuning
- **原理**：在输入前添加可训练前缀
- **特点**：仅训练前缀参数

#### Prompt Tuning
- **原理**：学习软提示词嵌入
- **特点**：简单高效

## 📊 微调方法对比
| 方法 | 参数量 | 训练速度 | 效果 | 部署 |
|---|---|---|---|---|
| **全参数** | 100% | 慢 | 最好 | 困难 |
| **LoRA** | 1% | 快 | 好 | 容易 |
| **Prefix** | 0.1% | 最快 | 中 | 容易 |