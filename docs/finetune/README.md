# 🔧 微调技术

## 🎯 微调概述
微调是将预训练模型适配到特定任务的关键技术，包括指令微调、对齐微调和高效参数微调。

## 🏗️ 微调类型

### 1️⃣ 指令微调 (SFT)
- **原理**：在指令-响应对上训练
- **数据格式**：{"instruction": "...", "output": "..."}
- **效果**：提升指令遵循能力

### 2️⃣ 对齐微调 (RLHF)
- **流程**：
  1. 人类偏好数据收集
  2. 奖励模型训练
  3. PPO强化学习优化
- **目标**：使模型行为符合人类价值观

### 3️⃣ 高效参数微调

#### LoRA (Low-Rank Adaptation)

#### 背景

矩阵中的最大的不相关的向量个数，就叫做**秩**。如果矩阵中数据比较相关，则为低秩矩阵。
- 例如，一个 $m \times n$ 的矩阵，如果它的秩 r 远小于 m 和 n，那么它就是一个低秩矩阵，冗余信息很多
- 反之，若矩阵的秩等于矩阵的行数 m，或者列数 n，则它是一个满秩矩阵
- 大模型的参数矩阵中的秩往往较小，具有很强的冗余

#### LoRA 原理

- **原理**：如果一个矩阵P（假设维度为$d \times d$ ）存在大量冗余信息，即低秩特性，可以不用完整的 $d \times d$ 尺寸来表示它，可利用因式分解这个想法，用两个较小的矩阵（ A和B ）的乘积 BA 来表示矩阵P，其中 A 维度 $r \times d$ ，B 维度 $d \times r$ ，则 $P = AB$ ，其中 $ r$ 为秩，远小于 $ d $ ，A 和 B 为低秩矩阵

- **思想**：将全参数微调理解为“冻住的预训练权重” + “微调过程中产生的权重更新量”，因此微调只学习“更新”的那部分参数量（与预训练权重维度相同）。同时借助矩阵分解的思想，将“更新”的大模型参数矩阵分解为两个低秩矩阵的乘积，从而减少参数量，提高训练速度
  
- **公式**：$output = W_0x + \Delta Wx = W_0x + BAx$ 

  其中 $W_0$ 为预训练权重，维度为 $d \times d$ ，$x$ 为输入，$B$ 和 $A$ 为可训练参数，维度分别为 $d \times r$，$r \times d$ ( $ r$ 为秩，远小于 $ d $ ), $\Delta W$ 为微调过程中产生的权重更新量，即$BA$ 

- **效果**：参数更新量由 $d \times d$ 降低为 $ 2 \times d \times r$ ，反向传播时，只有 A、B 获得梯度，$W_0$ 不变，因而大大减少计算量

#### LoRA的初始化

常见的初始化方法是：矩阵A 高斯初始化，矩阵 B 初始化为零矩阵，保证模型输出在第一步与预训练一致，避免任何初始扰动，保证不会在一开始就破坏预训练表示

- 为什么A 高斯初始化，B 初始化为零矩阵？为什么不能都高斯初始化？或者都为0，或者B 高斯初始化,A 初始化为零矩阵？
  - **A 随机与 B 零初始化：** B在第一轮即可获得梯度，更新参数，A在第一轮无法获得梯度，不更新参数，但随着B的更新，A会逐渐获得梯度，开始更新
  - **A 和 B 都高斯初始化：** 模型输出在第一步与预训练不一致，引入噪声，干扰训练
  - **A 和 B 都初始化为零：** A 和 B 均无法获得梯度，无法更新，训练无法启动
  - **B 随机与 A 零初始化：** 理论上可能，但会导致训练效率显著下降甚至失败。B在第一轮无法获得梯度，无法更新参数，A 在第一轮的梯度不为零（取决于随机初始化的 B），可以更新，但 A 的更新需要与 B 的更新协同才能有效调整权重。由于 B 在初始阶段无法更新，A 的更新方向会受限于初始随机的 B，导致优化过程不稳定。
  
  
  具体可参考： [LoRA微调中的矩阵初始化策略：A随机与B零初始化](https://ryojerryyu.github.io/blog-next/learn_from_ai/lora-matrix-initialization-strategy)
  
#### LoRA作用的位置

理论上LoRA的思想可以应用到任何权值矩阵上，例如在自注意机制中有四个权值矩阵  wq，wk ，wv ，wo ，另外在Transformer的全连接中也有两个权值矩阵w_up 和 w_down。关于LoRA在Transformer的作用位置，LoRA论文在自注意力层做了一组对照实验，证明如果只将LoRA作用到某个单一矩阵上，效果不佳，如果将LoRA作用到两个矩阵上，放在 wq 和 wv 效果最好。建议在所有的权值矩阵都加上LoRA，有利于模型捕捉到所有矩阵的关键信息


#### LoRA中参数
- $ r$ ：秩，表示低秩矩阵的维度，一般取1、2、4、8、16、32，比较常见的取值是8，值越高意味着矩阵越大
- lora_alpha：缩放因子，用于调整低秩矩阵的影响力。可以理解为对BA的更新幅度进行放大或缩小
- lora_target：指示需要应用低秩适应（LoRA）模块的特定网络层或模块

#### LoRA的变体

- **LoRA+（LoRA Plus）：** LoRA 的增强版本，主要通过为矩阵 A 和 B 引入不同的学习率改进Lora，其中矩阵 B 的学习率设置为矩阵 A 的 16 倍。这种策略可以显著提高训练效率，同时提升模型精度（约 2%），并将训练时间缩短 2 倍。前提：原始 LoRA 中，矩阵 A 和 B 使用相同的学习率进行更新。该方法认为当模型的宽度（即嵌入维度）较大时，这种单一学习率的设置会导致微调效果不佳。）

- **QLoRA（Quantized LoRA）**： QLoRA 是 LoRA 的量化版本，主要通过对低秩矩阵进行量化，从而显著降低存储和计算成本。这使得模型在显存受限的环境中运行更加高效。

- **AdaLoRA（Adaptive LoRA）**：AdaLoRA 是 LoRA 的自适应版本，它支持动态调整秩，可根据任务和数据的复杂度动态调整低秩矩阵的秩，避免了固定秩带来的限制。



#### Prefix Tuning
- **原理**：在输入前添加可训练前缀
- **特点**：仅训练前缀参数

#### Prompt Tuning
- **原理**：学习软提示词嵌入
- **特点**：简单高效

## 📊 微调方法对比
| 方法 | 参数量 | 训练速度 | 效果 | 部署 |
|---|---|---|---|---|
| **全参数** | 100% | 慢 | 最好 | 困难 |
| **LoRA** | 1% | 快 | 好 | 容易 |
| **Prefix** | 0.1% | 最快 | 中 | 容易 |