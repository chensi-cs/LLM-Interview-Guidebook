# 🏗️ 主流大模型结构

## 🎯 架构分类

### 1️⃣ Encoder-Decoder架构
- **代表模型**：T5、BART
- **特点**：适合序列到序列任务
- **应用**：翻译、摘要、问答

### 2️⃣ Decoder-Only架构
- **代表模型**：GPT系列、LLaMA
- **特点**：自回归生成
- **应用**：文本生成、对话

### 3️⃣ Encoder-Only架构
- **代表模型**：BERT、RoBERTa
- **特点**：双向理解
- **应用**：分类、NER、阅读理解

### 4️⃣ Prefix-Decoder架构
- **代表模型**：GLM、U-PaLM
- **特点**：编码器+部分解码器
- **应用**：兼顾理解和生成

## 🤖 主流模型详解

### GPT系列演进
| 模型 | 参数量 | 特点 | 发布时间 |
|---|---|---|---|
| **GPT-1** | 117M | 无监督预训练 | 2018 |
| **GPT-2** | 1.5B | 零样本能力 | 2019 |
| **GPT-3** | 175B | 少样本学习 | 2020 |
| **GPT-4** | ~1.7T | 多模态 | 2023 |

### LLaMA系列
- **LLaMA 1**：开源基础模型
- **LLaMA 2**：商业友好许可
- **LLaMA 3**：最强开源模型

### 中文大模型
- **Qwen**：阿里巴巴开源
- **DeepSeek**：深度求索
- **Kimi**：月之暗面
- **ChatGLM**：清华大学

## 📊 模型对比

| 模型 | 架构 | 参数量 | 上下文长度 | 特点 |
|---|---|---|---|---|
| **GPT-4** | Decoder | ~1.7T | 128K | 多模态、推理强 |
| **LLaMA-3** | Decoder | 70B | 8K | 开源、英文强 |
| **Qwen-72B** | Decoder | 72B | 32K | 中文优化 |
| **DeepSeek-67B** | Decoder | 67B | 32K | 数学推理 |

## 🎯 面试重点
1. **不同架构的优缺点？**
2. **GPT和BERT的区别？**
3. **如何选择合适的架构？**
4. **中文模型的特殊优化？**